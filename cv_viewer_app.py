import os
import pandas as pd
import streamlit as st
import fitz  # PyMuPDF
from docx import Document
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
import sqlite3
from datetime import datetime
import streamlit_authenticator as stauth
from io import BytesIO

# ======================= #
# --- Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ --- #
# ======================= #

# Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†
users = {
    "usernames": {
        "HRUSER": {
            "name": "HR User",
            "password": "Concordpassword78$"
        },
        "Sara": {
            "name": "Sara Samkari",
            "password": "Concordpassword78$"
        }
    }
}

# Ø¥Ø¹Ø¯Ø§Ø¯ ØµÙØ­Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
authenticator = stauth.Authenticate(
    users,
    "cv-analyzer",
    "abcdef",
    cookie_expiry_days=30,
    auto_hash=False  # Ø¨Ø¯ÙˆÙ† ØªØ´ÙÙŠØ± Ù…Ø¨Ø¯Ø¦ÙŠÙ‹Ø§ØŒ Ù†Ø£Ù…Ù†Ù‡ Ù„Ø§Ø­Ù‚Ù‹Ø§
)

# ÙˆØ§Ø¬Ù‡Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„
name, authentication_status, username = authenticator.login("Login", "main")

if authentication_status == False:
    st.error("âŒ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø£Ùˆ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± ØºÙŠØ± ØµØ­ÙŠØ­Ø©")
elif authentication_status == None:
    st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø§Ø³Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… ÙˆÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±")
elif authentication_status:
    st.success(f"Ù…Ø±Ø­Ø¨Ù‹Ø§ Ø¨ÙƒØŒ {name} ğŸ‘‹")
    
    # ======================= #
    # --- Ù‡Ù†Ø§ ÙŠØ¨Ø¯Ø£ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ --- #
    # ======================= #

    # --- ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ---
    df_train = pd.read_csv("training_data.csv")

    vectorizer = TfidfVectorizer(stop_words='english')
    X = vectorizer.fit_transform(df_train["Resume_Text"])
    y = df_train["Match"]

    model = LogisticRegression(class_weight='balanced')
    model.fit(X, y)

    # --- ØªØ¹Ø±ÙŠÙ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„ÙˆØ¸Ø§Ø¦Ù ---
    job_groups = {
        "ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª": ["Data Analyst", "Business Analyst", "Reporting Analyst"],
        "ØªØ·ÙˆÙŠØ± Ø¨Ø±Ù…Ø¬ÙŠØ§Øª": ["Software Developer", "Backend Engineer", "Full Stack Developer"],
        "Ø¯Ø¹Ù… ÙÙ†ÙŠ": ["IT Support", "Helpdesk", "Technical Support"],
        "Ù‡Ù†Ø¯Ø³Ø© Ù†Ø¸Ù…": ["Systems Engineer", "Infrastructure Engineer", "DevOps"],
        "Ø¥Ø¯Ø§Ø±Ø© Ù…Ø´Ø§Ø±ÙŠØ¹": ["Project Manager", "Project Coordinator", "Scrum Master"],
        "ÙˆØ¸Ø§Ø¦Ù Ø§Ù„ØªØ£Ù…ÙŠÙ†": ["Insurance Specialist", "Insurance Analyst", "Underwriter", "Claims Processor", "Insurance Coordinator"]
    }

    # --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ---
    df = pd.read_excel("batch_prediction_by_city.xlsx")

    st.title("ğŸ“‹ Ù„ÙˆØ­Ø© Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ø°Ø§ØªÙŠØ©")

    # --- Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø¯ÙŠÙ†Ø© ÙˆØ§Ù„ÙˆØ¸ÙŠÙØ© ---
    cities = sorted(df["City"].dropna().unique())
    selected_city = st.selectbox("ğŸ™ï¸ Ø§Ø®ØªØ± Ø§Ù„Ù…Ø¯ÙŠÙ†Ø©", cities)

    selected_job_group = st.selectbox("ğŸ§³ Ø§Ø®ØªØ± Ù†ÙˆØ¹ Ø§Ù„ÙˆØ¸ÙŠÙØ©", list(job_groups.keys()))
    related_titles = job_groups[selected_job_group]

    # --- ØªØµÙÙŠØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¹Ø±Ø¶Ù‡Ø§ ---
    filtered_df = df[
        (df["City"] == selected_city) &
        (df["Job_Title"].isin(related_titles))
    ]

    st.write(f"ğŸ” Ø¹Ø¯Ø¯ Ø§Ù„Ø³ÙŠØ± Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©: {len(filtered_df)}")
    st.dataframe(filtered_df)

    # --- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ€ Excel ---
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        filtered_df.to_excel(writer, index=False, sheet_name='Resumes')
    output.seek(0)

    st.download_button(
        label="ğŸ“¥ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙƒÙ€ Excel",
        data=output,
        file_name=f"{selected_city}_{selected_job_group}_resumes.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
    )

    # --- Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¬Ø¯ÙˆÙ„ Ø§Ù„ØªØ®Ø²ÙŠÙ† ---
    conn = sqlite3.connect("cv_results.db", check_same_thread=False)
    cursor = conn.cursor()
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS resumes (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            file_name TEXT,
            city TEXT,
            job_group TEXT,
            result TEXT,
            timestamp TEXT
        )
    ''')
    conn.commit()

    # --- Ø±ÙØ¹ ÙˆØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ±Ø© Ø°Ø§ØªÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© ---
    st.header("ğŸ“¤ ØªØ­Ù„ÙŠÙ„ Ø³ÙŠØ±Ø© Ø°Ø§ØªÙŠØ© Ø¬Ø¯ÙŠØ¯Ø©")

    uploaded_file = st.file_uploader("Ø§Ø±ÙØ¹ Ø³ÙŠØ±Ø© Ø°Ø§ØªÙŠØ© (PDF Ø£Ùˆ Word)", type=["pdf", "docx"])

    def extract_text(file):
        if file.name.endswith(".pdf"):
            text = ""
            with fitz.open(stream=file.read(), filetype="pdf") as doc:
                for page in doc:
                    text += page.get_text()
            return text
        elif file.name.endswith(".docx"):
            doc = Document(file)
            return "\n".join([para.text for para in doc.paragraphs])
        return None

    if uploaded_file:
        st.info(f"ğŸ“„ ØªÙ… Ø±ÙØ¹: {uploaded_file.name}")
        resume_text = extract_text(uploaded_file)

        if resume_text:
            vector = vectorizer.transform([resume_text])
            prediction = model.predict(vector)[0]
            result = "âœ… Ù…Ù†Ø§Ø³Ø¨" if prediction == 1 else "âŒ ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨"
            st.success(f"Ø§Ù„Ù†ØªÙŠØ¬Ø©: {result}")

            # --- Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
            cursor.execute('''
                INSERT INTO resumes (file_name, city, job_group, result, timestamp)
                VALUES (?, ?, ?, ?, ?)
            ''', (
                uploaded_file.name,
                selected_city,
                selected_job_group,
                result,
                datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            ))
            conn.commit()
            st.info("ğŸ“ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
        else:
            st.warning("âš ï¸ Ù„Ù… Ù†ØªÙ…ÙƒÙ† Ù…Ù† Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù†Øµ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù.")

    # --- Ù„ÙˆØ­Ø© Ø¹Ø±Ø¶ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---
    st.header("ğŸ“š Ø¹Ø±Ø¶ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©")

    df_db = pd.read_sql_query("SELECT * FROM resumes ORDER BY timestamp DESC", conn)
    st.dataframe(df_db)

